import numpy as np
import logging
import pickle
import argparse
import os
import time

from tqdm.auto import tqdm

from core.gen_models import (
    LocalModel, OpenAIModel, OpenAIChatModel, AzureOpenAIChatModel, GPT35Turbo0613ChatModel, GPT4Turbo20240409ChatModel,
    GPT4oMini20240718ChatModel, DeepSeekChatModel, GPT4oMinChatModel, DashScopeChatModel,
    Qwen2_7B_InstructChatModel, QwenPlusChatModel, QwenTurboChatModel, QwenMaxChatModel,
    LocalOpenAIChatModel, LocalQwenChatModel, LocalLlamaChatModel, LocalChatGLMChatModel
)
from core.players import (
    PersuadeeModel, PersuaderModel, P4GSystemPlanner,
    PersuaderChatModel, PersuadeeChatModel, P4GChatSystemPlanner
)
from core.esc_players import (
    TherapistModel, PatientModel, ESCSystemPlanner,
    TherapistChatModel, PatientChatModel, ESCChatSystemPlanner
)

from core.cima_palyers import (
    TeacherChatModel, StudentChatModel, CIMAChatSystemPlanner

)
from core.cb_players import (
    BuyerModel, SellerModel, CBSystemPlanner,
    BuyerChatModel, SellerChatModel, CBChatSystemPlanner
)
from core.game1 import EmotionalSupportGame, CBGame,CIMAGame
from core.game import PersuasionGame
from core.helpers import DialogSession, CBDialogSession, CIMADialogSession
from utils.utils import dotdict
from utils.prompt_examples import ESConv_EXP_DIALOG, CB_EXP_DIALOG, CIMA_EXP_DIALOG

from core.nrpa_cima import NRPAPlanner

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# ä¸ºæœ¬åœ°æ¨¡å‹è®¾ç½®æ—¥å¿—çº§åˆ«
logging.getLogger('core.gen_models').setLevel(logging.WARNING)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')


def main(cmd_args):
    # è®°å½•æ€»ä½“å¼€å§‹æ—¶é—´
    total_start_time = time.time()
    print(f"=== NRPA å®éªŒå¼€å§‹æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(total_start_time))} ===")

    system_name = CIMAGame.SYS
    user_name = CIMAGame.USR

    exp_1 = DialogSession(system_name, user_name).from_history(CIMA_EXP_DIALOG)

    game_ontology = CIMAGame.get_game_ontology()
    sys_da = game_ontology['system']['dialog_acts']
    user_da = game_ontology['user']['dialog_acts']

    if cmd_args.llm == 'code-davinci-002':
        backbone_model = OpenAIModel(cmd_args.llm)
        SysModel = BuyerModel
        UsrModel = SellerModel
        SysPlanner = CBSystemPlanner
    elif cmd_args.llm in ['gpt-3.5-turbo']:
        backbone_model = OpenAIChatModel(cmd_args.llm, cmd_args.gen_sentences)
        SysModel = TeacherChatModel
        UsrModel = StudentChatModel
        SysPlanner = CIMAChatSystemPlanner
    elif cmd_args.llm == 'chatgpt':
        backbone_model = AzureOpenAIChatModel(cmd_args.llm, cmd_args.gen_sentences)
        SysModel = BuyerChatModel
        UsrModel = SellerChatModel
        SysPlanner = CBChatSystemPlanner
    elif cmd_args.llm == 'gpt-3.5-turbo-0613':
        backbone_model = GPT35Turbo0613ChatModel(cmd_args.gen_sentences)
        SysModel = BuyerChatModel
        UsrModel = SellerChatModel
        SysPlanner = CBChatSystemPlanner
    elif cmd_args.llm == 'gpt-4-turbo-2024-04-09':
        backbone_model = GPT4Turbo20240409ChatModel(cmd_args.gen_sentences)
        SysModel = BuyerChatModel
        UsrModel = SellerChatModel
        SysPlanner = CBChatSystemPlanner
    elif cmd_args.llm == 'gpt-4o-mini-2024-07-18':
        backbone_model = GPT4oMini20240718ChatModel(cmd_args.gen_sentences)
        SysModel = BuyerChatModel
        UsrModel = SellerChatModel
        SysPlanner = CBChatSystemPlanner
    elif cmd_args.llm == 'gpt-4o-mini':
        backbone_model = GPT4oMinChatModel(cmd_args.gen_sentences)
        SysModel = TeacherChatModel
        UsrModel = StudentChatModel
        SysPlanner = CIMAChatSystemPlanner
    elif cmd_args.llm == 'deepseek-chat':
        backbone_model = DeepSeekChatModel(cmd_args.llm, cmd_args.gen_sentences)
        SysModel = BuyerChatModel
        UsrModel = SellerChatModel
        SysPlanner = CBChatSystemPlanner
    elif cmd_args.llm == 'qwen2-7b-instruct':
        backbone_model = Qwen2_7B_InstructChatModel(cmd_args.gen_sentences)
        SysModel = BuyerChatModel
        UsrModel = SellerChatModel
        SysPlanner = CBChatSystemPlanner
    elif cmd_args.llm == 'qwen2.5-7b-instruct':
        backbone_model = QwenPlusChatModel(cmd_args.gen_sentences)
        SysModel = BuyerChatModel
        UsrModel = SellerChatModel
        SysPlanner = CBChatSystemPlanner
    elif cmd_args.llm == 'qwen3-8b':
        backbone_model = QwenTurboChatModel(cmd_args.gen_sentences)
        SysModel = BuyerChatModel
        UsrModel = SellerChatModel
        SysPlanner = CBChatSystemPlanner
    elif cmd_args.llm == 'qwen3-0.6b':
        backbone_model = QwenMaxChatModel(cmd_args.gen_sentences)
        SysModel = TeacherChatModel
        UsrModel = StudentChatModel
        SysPlanner = CIMAChatSystemPlanner
    elif cmd_args.llm == 'local-openai':
        # é€šç”¨æœ¬åœ° OpenAI å…¼å®¹æ¨¡å‹
        backbone_model = LocalOpenAIChatModel(
            model_name=getattr(cmd_args, 'local_model_name', 'xxx'),
            base_url=getattr(cmd_args, 'local_base_url', 'http://localhost:6006/v1'),
            gen_sentences=cmd_args.gen_sentences
        )
        SysModel = TeacherChatModel
        UsrModel = StudentChatModel
        SysPlanner = CIMAChatSystemPlanner
    elif cmd_args.llm == 'local-qwen':
        # æœ¬åœ° Qwen æ¨¡å‹
        backbone_model = LocalQwenChatModel(
            gen_sentences=cmd_args.gen_sentences,
            base_url=getattr(cmd_args, 'local_base_url', 'http://localhost:6006/v1')
        )
        SysModel = TeacherChatModel
        UsrModel = StudentChatModel
        SysPlanner = CIMAChatSystemPlanner
    elif cmd_args.llm == 'local-llama':
        # æœ¬åœ° Llama æ¨¡å‹
        backbone_model = LocalLlamaChatModel(
            gen_sentences=cmd_args.gen_sentences,
            base_url=getattr(cmd_args, 'local_base_url', 'http://localhost:6006/v1')
        )
        SysModel = TeacherChatModel
        UsrModel = StudentChatModel
        SysPlanner = CIMAChatSystemPlanner
    elif cmd_args.llm == 'local-chatglm':
        # æœ¬åœ° ChatGLM æ¨¡å‹
        backbone_model = LocalChatGLMChatModel(
            gen_sentences=cmd_args.gen_sentences,
            base_url=getattr(cmd_args, 'local_base_url', 'http://localhost:6006/v1')
        )
        SysModel = TeacherChatModel
        UsrModel = StudentChatModel
        SysPlanner = CIMAChatSystemPlanner
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹: {cmd_args.llm}")

    system = SysModel(
        sys_da,
        backbone_model,
        conv_examples=[exp_1],
        inference_args={
            "temperature": 0.7,
            "do_sample": True,
            "return_full_text": False,
        },
        zero_shot=False
    )
    user = UsrModel(
        user_da,
        inference_args={
            "max_new_tokens": 128,
            "temperature": 1.1,
            "repetition_penalty": 1.0,
            "do_sample": True,
            "return_full_text": False,
        },
        backbone_model=backbone_model,
        conv_examples=[exp_1],
        zero_shot=False
    )

    planner = SysPlanner(
        dialog_acts=system.dialog_acts,
        max_hist_num_turns=system.max_hist_num_turns,
        user_dialog_acts=user.dialog_acts,
        user_max_hist_num_turns=user.max_hist_num_turns,
        generation_model=backbone_model,
        conv_examples=[exp_1],
        zero_shot=False
    )

    game = CIMAGame(system, user, planner, zero_shot=False)
    print(f"ä½¿ç”¨æ¨¡å‹: {cmd_args.llm}")
    print(f"ç³»ç»Ÿå¯¹è¯è¡Œä¸º: {system.dialog_acts}")
    print(f"ç”¨æˆ·å¯¹è¯è¡Œä¸º: {user.dialog_acts}")

    import json
    all_dialogs = {}
    with open(r"D:\GDPZero-master\data\cima-test.txt", "r", encoding="utf-8") as f:
        for line_num, line in enumerate(f):
            line = line.strip()
            if line:
                try:
                    dialog_data = json.loads(line)
                    dialog_id = f"dialog_{line_num}"
                    all_dialogs[dialog_id] = dialog_data
                except json.JSONDecodeError as e:
                    print(f"è·³è¿‡ç¬¬ {line_num} è¡Œï¼ŒJSONè§£æé”™è¯¯: {e}")
                    continue

    num_dialogs = cmd_args.num_dialogs

    nrpa_args = dotdict({
        "nrpa_depth": cmd_args.nrpa_depth,
        "nrpa_iterations": cmd_args.reduced_iterations if cmd_args.reduced_iterations > 0 else cmd_args.nrpa_iterations,
        "nrpa_playout_epsilon": cmd_args.nrpa_playout_epsilon,
        "max_playout_steps": cmd_args.max_playout_steps,
        "early_stopping_enabled": cmd_args.early_stopping_enabled,
        "early_stopping_threshold": cmd_args.early_stopping_threshold,
        "early_stopping_patience": cmd_args.early_stopping_patience,
        "min_iterations": cmd_args.min_iterations,
        "debug": cmd_args.debug,
    })
    
    print(f"è°ƒè¯•è¾“å‡º: å·²å¯ç”¨")

    output = []
    processed_dialogs = set()
    if os.path.exists(cmd_args.output):
        try:
            with open(cmd_args.output, "rb") as f:
                output = pickle.load(f)
                print(f"å·²åŠ è½½ç°æœ‰è¾“å‡ºæ–‡ä»¶ï¼ŒåŒ…å« {len(output)} æ¡è®°å½•")
                for item in output:
                    if 'did' in item:
                        processed_dialogs.add(item['did'])
                print(f"å·²å¤„ç†çš„å¯¹è¯ID ({len(processed_dialogs)} ä¸ª): {processed_dialogs}")
        except Exception as e:
            print(f"è¯»å–è¾“å‡ºæ–‡ä»¶å¤±è´¥: {e}. å°†åˆ›å»ºæ–°çš„è¾“å‡ºæ–‡ä»¶.")
    else:
        print(f"è¾“å‡ºæ–‡ä»¶ {cmd_args.output} ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶")

    bad_dialogs = [
        # Add any problematic CIMA dialog IDs here if needed
    ]

    dialog_keys_to_process = [k for k in all_dialogs.keys() if k not in bad_dialogs]
    target_dialogs_count = min(num_dialogs, len(dialog_keys_to_process))
    needed_new_dialogs = target_dialogs_count - len(processed_dialogs)

    if needed_new_dialogs <= 0:
        print(f"å·²å®Œæˆæ‰€æœ‰ {target_dialogs_count} ä¸ªç›®æ ‡å¯¹è¯çš„å¤„ç†")
        return

    print(
        f"ç›®æ ‡å¤„ç† {target_dialogs_count} ä¸ªå¯¹è¯ï¼Œå½“å‰å·²å¤„ç† {len(processed_dialogs)} ä¸ªï¼Œå°†å¤„ç† {needed_new_dialogs} ä¸ªæ–°å¯¹è¯")

    num_done = 0
    pbar = tqdm(total=needed_new_dialogs, desc="Evaluating")

    total_turns = 0
    successful_dialogs = 0
    dialog_turn_counts = []
    successful_turn_counts = []

    dialog_count = 0
    for did in dialog_keys_to_process:
        if did in processed_dialogs:
            continue
        if num_done >= needed_new_dialogs:
            break

        dialog_count += 1
        if hasattr(cmd_args, 'start_dialog') and dialog_count < cmd_args.start_dialog:
            print(f"è·³è¿‡å¯¹è¯ {dialog_count}: {did}")
            continue

        print(f"\næ­£åœ¨è¯„ä¼°å¯¹è¯ID: {did} ({num_done + 1}/{needed_new_dialogs})")
        dialog = all_dialogs[did]

        # CIMAå¯¹è¯åœºæ™¯çš„æ•°æ®ç»“æ„
        sentence = dialog.get("sentence", "")
        target = dialog.get("target", "")
        existing_dialog = dialog.get("dialog", [])

        logger.info("evaluating dialog sentence: {}".format(sentence))
        
        # å°†ç°æœ‰å¯¹è¯è½¬æ¢ä¸ºå†å²æ ¼å¼ - åªå–å‰ä¸¤å¥ï¼ŒæŒ‰ç…§env.pyçš„é€»è¾‘
        history = []
        if existing_dialog and len(existing_dialog) >= 2:
            # åªå–å‰ä¸¤å¥å¯¹è¯
            first_turn = existing_dialog[0]  # ç³»ç»Ÿçš„ç¬¬ä¸€å¥
            second_turn = existing_dialog[1]  # ç”¨æˆ·çš„ç¬¬ä¸€å¥å›åº”
            
            if first_turn.get("speaker") == "sys":
                history.append((CIMAGame.SYS, CIMAGame.S_Others, first_turn.get("text", "")))
            
            if second_turn.get("speaker") == "usr":
                history.append((CIMAGame.USR, CIMAGame.U_DidNotTry, second_turn.get("text", "")))
        
        initial_state = game.init_dialog(sentence, target, history)

        # æŒ‰ç…§CIMAçš„åˆå§‹å¯¹è¯åœºæ™¯
        sys_role = CIMAGame.SYS
        usr_role = CIMAGame.USR
        
        # å¦‚æœæ²¡æœ‰ç°æœ‰çš„å¯¹è¯å†å²ï¼Œåˆ›å»ºåˆå§‹å¯¹è¯
        if not history:
            initial_history = [(sys_role, CIMAGame.S_Others, f"Please translate \"{sentence}\" into Italian.")]
            initial_state = game.init_dialog(sentence, target, initial_history)
        
        # ä»å†å²è®°å½•ä¸­è·å–ä¸Šä¸‹æ–‡ç”¨äºæ‰“å°
        if initial_state.history:
            sys_utt = initial_state.history[0][2]
            if len(initial_state.history) > 1:
                usr_utt = initial_state.history[1][2]
            else:
                usr_utt = ""
        else:
            sys_utt = f"Please translate \"{sentence}\" into Italian."
            usr_utt = ""
        
        end_condition = CIMAGame.U_Correct

        context = f"""
        {sys_role}: {sys_utt}
        {usr_role}: {usr_utt}
        """
        initial_context = context.replace('\t', '').strip()
        print(f"\n=== å¼€å§‹æ¨¡æ‹Ÿå¯¹è¯ {did} ===")
        print(f"åˆå§‹å¯¹è¯ä¸Šä¸‹æ–‡:\n{initial_context}\n" + "=" * 50)
        print(f"è¦ç¿»è¯‘çš„å¥å­: {sentence}")
        print(f"ç›®æ ‡ç¿»è¯‘: {target}")

        # æ¸…ç†ç¼“å­˜
        if hasattr(backbone_model, '_cached_generate'):
            backbone_model._cached_generate.cache_clear()
        if hasattr(system, '_cached_generate'):
            system._cached_generate.cache_clear()
        if hasattr(user, '_cached_generate'):
            user._cached_generate.cache_clear()
        if hasattr(planner, '_cached_generate'):
            planner._cached_generate.cache_clear()

        # --- ä¸€æ¬¡æ€§å®Œæ•´å¯¹è¯æ¨¡æ‹Ÿ ---
        print(f"å¼€å§‹NRPAæœç´¢ (æ·±åº¦={nrpa_args.nrpa_depth}, è¿­ä»£={nrpa_args.nrpa_iterations})...")
        nrpa_start_time = time.time()
        dialog_planner = NRPAPlanner(game, planner, nrpa_args)
        final_state = dialog_planner.nrpa(nrpa_args.nrpa_depth, {}, initial_state.copy())
        nrpa_duration = time.time() - nrpa_start_time
        print(f"NRPAæœç´¢å®Œæˆ! è€—æ—¶: {nrpa_duration:.2f}ç§’")

        # --- å¤„ç†æ¨¡æ‹Ÿç»“æœ ---
        if final_state and len(final_state.history) > len(initial_state.history):
            print("\n--- å¯¹è¯æ¨¡æ‹Ÿè¯¦ç»†è¿‡ç¨‹ ---")
            simulated_turns = final_state.history[len(initial_state.history):]
            current_context = sys_utt + "\n" + usr_utt
            is_solved = False

            turn_count_in_sim = 0
            for i in range(0, len(simulated_turns), 2):
                turn_count_in_sim += 1

                sys_turn = simulated_turns[i]
                sys_da, sys_resp = sys_turn[1], sys_turn[2]

                if i + 1 < len(simulated_turns):
                    usr_turn = simulated_turns[i + 1]
                    usr_da, usr_resp = usr_turn[1], usr_turn[2]
                else:
                    # å¯¹è¯ä»¥ç³»ç»Ÿå›åº”ç»“æŸ
                    usr_da, usr_resp = "N/A", ""

                print(f"\n--- æ¨¡æ‹Ÿè½®æ¬¡: {turn_count_in_sim} ---")
                print(f"Teacher: [{sys_da}] {sys_resp}")
                print(f"Student: [{usr_da}] {usr_resp}")

                current_context += f"\nTeacher: {sys_resp}\nStudent: {usr_resp}"

                cmp_data = {
                    'did': did,
                    'turn': turn_count_in_sim,
                    'context': current_context.strip(),
                    'new_resp': sys_resp,
                    'new_da': sys_da,
                    'usr_resp': usr_resp,
                    'usr_da': usr_da,
                    'sentence': sentence,
                    'target': target,
                    "debug": {
                        "nrpa_iterations": nrpa_args.nrpa_iterations,
                        "nrpa_depth": nrpa_args.nrpa_depth,
                        "nrpa_search_time": nrpa_duration,
                    }
                }
                output.append(cmp_data)

                if usr_da == CIMAGame.U_Correct:
                    is_solved = True
                    break

            print("-" * 50)

            if is_solved:
                print(f"\nğŸ‰ å¯¹è¯ {did} åœ¨ç¬¬ {turn_count_in_sim} è½®ç»“æŸ (å­¦ç”Ÿæ­£ç¡®ç¿»è¯‘)!")
                successful_dialogs += 1
                successful_turn_counts.append(turn_count_in_sim)
            else:
                print(f"\nâŒ å¯¹è¯ {did} æ¨¡æ‹Ÿç»“æŸæ—¶å­¦ç”Ÿæœªèƒ½æ­£ç¡®ç¿»è¯‘ (å…± {turn_count_in_sim} è½®)")

            dialog_turn_counts.append(turn_count_in_sim)
            total_turns += turn_count_in_sim

        else:
            print("è­¦å‘Š: NRPAæœªèƒ½ç”Ÿæˆæœ‰æ•ˆå¯¹è¯ã€‚")
            dialog_turn_counts.append(0)

        with open(cmd_args.output, "wb") as f:
            pickle.dump(output, f)

        processed_dialogs.add(did)
        num_done += 1
        pbar.update(1)

    pbar.close()

    total_end_time = time.time()
    total_duration = total_end_time - total_start_time
    print(f"\n=== NRPA å®éªŒå®Œæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(total_end_time))} ===")
    print(f"=== æ€»è¿è¡Œæ—¶é—´: {total_duration:.2f}ç§’ ({total_duration / 60:.2f}åˆ†é’Ÿ) ===")
    if num_done > 0:
        print(f"=== å¹³å‡æ¯ä¸ªå¯¹è¯å¤„ç†æ—¶é—´: {total_duration / num_done:.2f}ç§’ ===")

    if num_done > 0:
        success_rate = (successful_dialogs / num_done) * 100
        average_turns = total_turns / num_done
        print(f"\n=== å¯¹è¯ç»Ÿè®¡ç»“æœ ===")
        print(f"æ€»å¯¹è¯æ•°: {num_done}")
        print(f"æˆåŠŸå­¦ä¼šç¿»è¯‘çš„å¯¹è¯æ•°: {successful_dialogs}")
        print(f"æˆåŠŸç‡: {success_rate:.1f}% ({successful_dialogs}/{num_done})")
        print(f"å¹³å‡è½®æ•°: {average_turns:.1f}è½®")
        print(f"å„å¯¹è¯è½®æ•°åˆ†å¸ƒ: {dialog_turn_counts}")
        if successful_dialogs > 0:
            avg_successful_turns = sum(successful_turn_counts) / len(successful_turn_counts)
            print(f"æˆåŠŸå­¦ä¹ çš„å¹³å‡è½®æ•°: {avg_successful_turns:.1f}è½®")
            print(f"æˆåŠŸå­¦ä¹ çš„è½®æ•°åˆ†å¸ƒ: {successful_turn_counts}")

    print(f"\næ‰€æœ‰å¯¹è¯å¤„ç†å®Œæˆã€‚å…±å¤„ç† {len(processed_dialogs)} ä¸ªä¸åŒå¯¹è¯IDã€‚æ€»è®°å½•æ•°: {len(output)}")
    with open(cmd_args.output, "wb") as f:
        pickle.dump(output, f)
    print(f"æœ€ç»ˆç»“æœå·²ä¿å­˜åˆ°: {cmd_args.output}")
    return


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--output', type=str,
                        default=r"D:\GDPZero-master\outputs\gpt-3.5-turbo_nrpa_level2_CIMA_test.pkl",
                        help='output file path')
    parser.add_argument('--llm', type=str, default="gpt-3.5-turbo",
                        choices=["code-davinci-002", "gpt-3.5-turbo", "chatgpt", "gpt-3.5-turbo-0613",
                                 "gpt-4-turbo-2024-04-09", "gpt-4o-mini-2024-07-18", "deepseek-chat", "gpt-4o-mini",
                                 "qwen2-7b-instruct", "qwen2.5-7b-instruct", "qwen3-8b", "qwen3-0.6b",
                                 "local-openai", "local-qwen", "local-llama", "local-chatglm"],
                        help='LLM backbone model name')
    parser.add_argument('--gen_sentences', type=int, default=-1, help='max number of sentences for LLM generation')
    parser.add_argument('--nrpa_depth', type=int, default=2, help='NRPA recursive search depth. 0 for pure playout.')
    parser.add_argument('--nrpa_iterations', type=int, default=3, help='Number of iterations per NRPA level')
    parser.add_argument('--num_dialogs', type=int, default=130, help='Target number of dialogs to process')
    parser.add_argument('--nrpa_playout_epsilon', type=float, default=0,
                        help='Epsilon for epsilon-greedy exploration in NRPA playouts')
    parser.add_argument('--reduced_iterations', type=int, default=0,
                        help='Reduced number of iterations (overrides nrpa_iterations if > 0)')
    parser.add_argument('--max_playout_steps', type=int, default=10, help='Maximum playout steps (0 means unlimited)')
    parser.add_argument('--start_dialog', type=int, default=1, help='Start processing from dialog number (1-based)')
    parser.add_argument('--early_stopping_enabled', type=bool, default=True, help='Enable early stopping mechanism')
    parser.add_argument('--early_stopping_threshold', type=int, default=2,
                        help='Early stopping threshold (dialog turns)')
    parser.add_argument('--early_stopping_patience', type=int, default=3, help='Early stopping patience')
    parser.add_argument('--min_iterations', type=int, default=1, help='Minimum iterations')
    parser.add_argument('--debug', action='store_true', help='debug mode')
    
    # æœ¬åœ°æ¨¡å‹ç›¸å…³å‚æ•°
    parser.add_argument('--local_base_url', type=str, default='http://localhost:6006/v1', 
                        help='Base URL for local OpenAI-compatible API')
    parser.add_argument('--local_model_name', type=str, default='xxx', 
                        help='Model name for local OpenAI-compatible API')

    cmd_args = parser.parse_args()

    # å®‰å…¨æ£€æŸ¥ï¼šå¦‚æœæ·±åº¦ä¸º0ï¼Œå¼ºåˆ¶è¿›è¡Œplayout
    if cmd_args.nrpa_depth == 0:
        print("è­¦å‘Š: NRPA æ·±åº¦ä¸º0ï¼Œå°†ä»…ä½¿ç”¨ playout è¿›è¡Œæ¨¡æ‹Ÿï¼Œæ— é€’å½’æœç´¢ã€‚")

    print("\nå‘½ä»¤è¡Œå‚æ•°:")
    print(f"  Output file: {cmd_args.output}")
    print(f"  LLM Model: {cmd_args.llm}")
    print(f"  Max Gen Sentences: {cmd_args.gen_sentences}")
    print(f"  NRPA Depth: {cmd_args.nrpa_depth}")
    print(f"  NRPA Iterations: {cmd_args.nrpa_iterations}")
    print(f"  Num Dialogs: {cmd_args.num_dialogs}")
    print(f"  NRPA Playout Epsilon: {cmd_args.nrpa_playout_epsilon}")
    print(f"  Max Playout Steps: {cmd_args.max_playout_steps}")
    print(f"  Start Dialog: {cmd_args.start_dialog}")
    if cmd_args.reduced_iterations > 0:
        print(f"  Using Reduced Iterations: {cmd_args.reduced_iterations}")
    if cmd_args.early_stopping_enabled:
        print(
            f"  Early Stopping: Enabled, Threshold={cmd_args.early_stopping_threshold}, Patience={cmd_args.early_stopping_patience}, Minimum Iterations={cmd_args.min_iterations}")
    
    # æ˜¾ç¤ºæœ¬åœ°æ¨¡å‹é…ç½®
    if cmd_args.llm.startswith('local-'):
        print(f"  Local Model Base URL: {cmd_args.local_base_url}")
        print(f"  Local Model Name: {cmd_args.local_model_name}")

    main(cmd_args)
